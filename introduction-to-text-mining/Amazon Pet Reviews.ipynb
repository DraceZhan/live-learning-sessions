{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import urllib.request\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data sourced from http://jmcauley.ucsd.edu/data/amazon/\n",
    "#let's examine how a json file looks!\n",
    "\n",
    "with urllib.request.urlopen('https://graderdata.s3.amazonaws.com/reviews_Pet_Supplies_5.json') as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(line) for line in data]\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('https://graderdata.s3.amazonaws.com/reviews_Pet_Supplies_5.json', lines=True)\n",
    "#lines = True is for parsing more than one block of data from your json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First there seems to be some \"default tags\", let's see how many of those exists in our data sets\n",
    "\n",
    "df[df['reviewerName'].str.contains('Consumer')]\n",
    "\n",
    "#notice one issue commonly encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#to fix it\n",
    "\n",
    "consumer_df = df[df['reviewerName'].str.contains('Consumer', na=False)]\n",
    "consumer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When it comes to review data, generally there wil be skewness and other potential issues, let's evaluate to see if it's true here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.overall.astype('str').value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's evaluate what might be the most frequent words observed in 5's as it's by far our most popular rating. We'll combine a few steps at once here.\n",
    "\n",
    "- First we'll create a mask for our data frame. Then we'll lower case all the string text found within our review as well as join all our text into a single long string to form what's known as a \"corpus\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_rev_corpus = ' '.join(df[df['overall']==5]['reviewText']).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's introduce some tools that will assist us with counting our most frequent words (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_filters = stopwords.words('english') + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rev_tokens = [lemmatizer.lemmatize(tokens) for tokens in word_tokenize(best_rev_corpus) if tokens not in stop_filters]\n",
    "\n",
    "fdist = FreqDist(best_rev_tokens)\n",
    "fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next steps:**\n",
    "\n",
    "- So we see here that there's still some tokens that are not very helpful. While you may be interested in how often dog and cat appears, it's unlikely that we can attribute those tokens to the five star reviews. Since this will be an iterative process, it makes sense for us to create a function that will filter for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## note this uses word_tokenize from nltk\n",
    "\n",
    "def extra_filter(corpus, stop_tokens):\n",
    "    '''\n",
    "    corpus: string format of text data\n",
    "    stop_tokens: list of tokens you wish to add to stopwords filter\n",
    "    '''\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    stop_filters = stopwords.words('english') + list(string.punctuation) + stop_tokens\n",
    "    filtered_tokens = [lemmatizer.lemmatize(tokens) for tokens in word_tokenize(corpus) \n",
    "                       if tokens not in stop_filters]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extra_stopwords = [\"n't\", \"'s\", 'dog', 'cat', '...' ,\"''\", \"'m\", '``', '--', 'pet']\n",
    "\n",
    "best_rev_new_toks = extra_filter(corpus= best_rev_corpus, stop_tokens=extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(best_rev_new_toks)\n",
    "fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The results are better but we may still be missing some of the context of what people are talking about. The issue is, we're currently examining strictly tokens in isolation, but what if we can capture some of the context behind each token?\n",
    "\n",
    "- One method to do so is by extracting bigrams instead of individual words (unigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_rev_bigram = list(nltk.bigrams(best_rev_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist_bi = FreqDist(best_rev_bigram)\n",
    "fdist_bi.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So already just from examining this list, we can see some potential comments rise towards the top namely that products where the animal seem to enjoy them influences a good nature of the five star reviews. In addition, other traits such as easy to use for the owner also factor in.\n",
    "\n",
    "- We can do additional iterations to filter out excess \"obvious\" factors but we will leave that direction for additional future work.\n",
    "\n",
    "- Instead, let's examine a different library that can also replicate our desired effect of examining most frequent bigrams.\n",
    "\n",
    "- Ie. different tools for similar effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt_vec = CountVectorizer(ngram_range=(2,2), stop_words='english',max_features=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vec.fit_transform(df[df['overall']==5]['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's tie this together with other topics we've learned from Data Analysis. For example, suppose, we're interested in the top 5 terms and their frequencies of each of the review ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First we create the groups into groupby objects\n",
    "\n",
    "rev_groups = df.groupby('overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#next let's build a function that we can apply aggregated to our groups\n",
    "\n",
    "def freq_analysis(txt, stop_tokens=extra_stopwords, num =50):\n",
    "    txt = ' '.join(txt).lower()\n",
    "    stop_filters = stopwords.words('english') + list(string.punctuation) + stop_tokens\n",
    "    filtered_tokens = [tokens for tokens in word_tokenize(txt) \n",
    "                       if tokens not in stop_filters]\n",
    "    \n",
    "    return filtered_tokens\n",
    "    fdist = FreqDist(filtered_tokens)\n",
    "    return fdist.most_common(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rev_top50 = rev_groups.agg({'reviewText' : freq_analysis})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "rev_top50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While we still need to dig deeper, we can see that there does seem to be some trend where dog products seem to review worse than cats from even the review tokens themselves. A fair amount of negative reviews seems related to dog food or the after effects said food products. \n",
    "\n",
    "- Next as a pet owner, I personally will be interested in certain aspects of a product, namely safety as a feature.\n",
    "\n",
    "- During the session covered in the program, there is a lecture labeled \"Applications with NLP\" where we examine latent \"topics\" that can provide some of this information but we'll employ a more \"basic\" method here.\n",
    "\n",
    "- By looking for the word \"allergies\" we may be interested in what words appear in context with that term within our reviews. Those words may be of interest for us to be aware of, for potential allergens to either our pet or ourselves.\n",
    "\n",
    "- For processessing time of certain tasks later, we'll just use the five star reviews for now to demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a nltk Text object\n",
    "\n",
    "best_rated_text = nltk.Text(best_rev_new_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rated_text.concordance('allergies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yikes some nasty stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can examine contexts of two terms if needed\n",
    "\n",
    "best_rated_text.common_contexts(['allergies', 'cats'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From here we're able to find some common themes regarding products that seem to review high regarding this \"topic\" of interest. The reviews that discuss these products are often centered around food allergies. So those may be something to examine when reading reviews of interest for a product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So now at this stage, we've only scratched the surface of what we can examine with our standard data analytics tools from text. Try to see how you can answer the following questions?\n",
    "    - Which products appear to be the most safe according to your analysis? (hint are there proxies of information you can use for this?)\n",
    "    - Can you find durability ratings for certain toys?\n",
    "    - Are there products that pets especially seem to like? Dislike?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
